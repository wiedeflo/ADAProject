{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "File name: project.ipynb\n",
    "Author: ...Jose, Mohamed Ndoye, Raphael Strebel\n",
    "Date created: 03/11/2019\n",
    "Date last modified: ...\n",
    "Python Version: 3.7.4\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"up\"></a>\n",
    "# Food Inspections in Chicago\n",
    "\n",
    " - [Load Databases](#load-databases)\n",
    " - [Complete Datasets](#complete-datasets)\n",
    " - [Maps](#maps)\n",
    " - [Basic Statistics](#basic-stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful : https://www.sustainabilist.com/blog/chicago-data-analysis-a-internship-project\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# TODO : Add to README \"download libraries geopandas, vincent,...\" with a short description to explain its use\n",
    "import geopandas as gpd\n",
    "\n",
    "import vincent\n",
    "vincent.core.initialize_notebook() \n",
    "\n",
    "from utils import constants as cst\n",
    "from utils import clean_database\n",
    "from utils import areas_handler\n",
    "from utils import chicago_map_handler as maps\n",
    "\n",
    "\n",
    "import folium\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Set auto-reload \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'load-databases'></a>\n",
    "## Load Databases\n",
    "\n",
    "In this section we load and clean the databases.\n",
    "\n",
    "[Table of Contents](#up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the areas dataframe \n",
    "areas_DF = gpd.read_file(cst.AREAS_PATH)\n",
    "\n",
    "# Clean the dataframe\n",
    "areas_DF = clean_database.clean_areas_df(areas_DF)\n",
    "areas_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the food inspections dataframe\n",
    "food_inspections_DF = pd.read_csv(cst.FOOD_INSPECTIONS_PATH, sep = ',', header = 0, \n",
    "                   names = cst.FOOD_INSPECTIONS_COL_NAMES, index_col = None, error_bad_lines=False\n",
    "                   )\n",
    "\n",
    "# Clean the dataframe\n",
    "food_inspections_DF = clean_database.clean_food_inspections_df(food_inspections_DF, areas_DF)\n",
    "food_inspections_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the socio-economic indicators dataframe\n",
    "socio_economic_DF = pd.read_csv(cst.SOCIO_ECONOMIC_INDICATORS_PATH, sep = ',', header = 0, \n",
    "                   names = cst.SOCIO_ECONOMIC_COL_NAMES, index_col = None, error_bad_lines=False\n",
    "                   )\n",
    "\n",
    "# Clean the dataframe\n",
    "socio_economic_DF = clean_database.clean_socio_economic_df(socio_economic_DF)\n",
    "socio_economic_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the life expectancy dataframe\n",
    "life_expectancy_DF = pd.read_csv(cst.LIFE_EXPECTANCY_PATH, sep = ',', header = 0, \n",
    "                   names = cst.LIFE_EXPECTANCY_COL_NAMES, index_col = None, error_bad_lines=False\n",
    "                   )\n",
    "\n",
    "# Clean the dataframe\n",
    "life_expectancy_DF = clean_database.clean_socio_economic_df(life_expectancy_DF)\n",
    "life_expectancy_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = 'complete-datasets'></a>\n",
    "## Complete Datasets\n",
    "\n",
    "### 2 problems : \n",
    "1. we only have the area name for the life exp. and the socio-eco DFs -> find the regions in sequence of lat/lng pairs that corresponds to the boundaries of an area. Then we can determine the region of the facility of the food_inspections dataframe and work only with the regions for the rest of the project (thoughts ?).\n",
    "2. some entries in food_inspections_DF have no lat/lng pair -> must find it given their address\n",
    "\n",
    "[Table of Contents](#up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge socio-economic and life expectancy df's on the area number and names\n",
    "socio_life_merged_DF = socio_economic_DF.merge(life_expectancy_DF, how=\"left\", on=[\"community_area_num\", \"community_area_name\"])# [[\"community_area_num\", \"community_area_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socio_life_merged_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter locations where lng/lat are unknown\n",
    "food_unknown_loc = food_inspections_DF[food_inspections_DF['lat'].isna()]\n",
    "\n",
    "#consider only those locations that are actually in the city of Chicago (or unknown) \n",
    "food_unknown_loc = food_unknown_loc[(food_unknown_loc['city'].str.lower() == 'chicago') | (food_unknown_loc['city'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unknown locations \n",
    "unknown_locations = areas_handler.get_unknown_locations(food_unknown_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the locations not found by OpenStreetMaps\n",
    "unknown_locations[pd.isnull(unknown_locations['lat'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the Chicago areas\n",
    "# This map contains additional layers to visually check if the found locations are actually within the borders of the city\n",
    "map_chicago = maps.create_chicago_map()\n",
    "map_chicago = maps.add_locations(map_chicago, unknown_locations, food_inspections_DF) \n",
    "map_chicago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of the map**\n",
    "\n",
    "On the map above we can see the city of Chicago and the region where the facilities of the inspections presented in the food_inspections dataset are located. In the following, we will focus ang get some insights on that area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use unknonwn_locations to fill lat and lng in the original dataframe food_inspections_DF\n",
    "\n",
    "food_unknown_loc = food_unknown_loc.reset_index().merge(unknown_locations, on=\"address\", how='left').set_index('index')\n",
    "food_unknown_loc = food_unknown_loc.drop(['lat_x', 'lng_x'], axis = 1)\n",
    "food_unknown_loc = food_unknown_loc.rename(columns={'lat_y':'lat','lng_y':'lng'})\n",
    "\n",
    "food_inspections_DF.update(food_unknown_loc)\n",
    "food_inspections_DF[food_inspections_DF['lat'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve are numbers and delete unknown areas\n",
    "# this takes a while\n",
    "\n",
    "food_inspections_DF = areas_handler.get_locations_with_area(food_inspections_DF, areas_DF)\n",
    "print(\"Number of locations: \" + str(food_inspections_DF.shape[0]))\n",
    "#drop locations not in the city of chicago \n",
    "food_inspections_DF = food_inspections_DF.dropna(subset=[cst.AREA_NUM])\n",
    "print(\"Number of locations in the city of chicaco: \" + str(food_inspections_DF.shape[0]))\n",
    "food_inspections_DF[cst.AREA_NUM] = food_inspections_DF[cst.AREA_NUM].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'maps'></a>\n",
    "## Maps\n",
    "\n",
    "We visualize some of the data using folium to find connections\n",
    "\n",
    "[Table of Contents](#up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of inspections map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO there is a SettingWithCopyWarning: \n",
    "# A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "# Try using .loc[row_indexer,col_indexer] = value instead\n",
    "\n",
    "#create new dataframe with number of inspections per area\n",
    "inspection_counts = food_inspections_DF[cst.AREA_NUM].value_counts().to_frame()\n",
    "inspection_counts.reset_index(level=0, inplace=True)\n",
    "inspection_counts.columns=[cst.AREA_NUM,'num_inspections']\n",
    "inspection_counts[cst.AREA_NUM] = inspection_counts[cst.AREA_NUM].astype(str)\n",
    "inspection_counts.sort_values('num_inspections');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heatmap_chicago = maps.heat_map(inspection_counts, \"Inspections\", cst.AREA_NUM, 'num_inspections')\n",
    "heatmap_chicago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of the map**\n",
    "\n",
    "The heatmap above shows the number of inspections on the area we are focusing on and determined before. We can see that the more we approach the city center of Chicago, the higher the number of inspections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Establishments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_DF = food_inspections_DF.copy()\n",
    "count_DF = count_DF.drop_duplicates(subset=['license_num'])\n",
    "count_ser = count_DF[cst.AREA_NUM].value_counts().to_frame()\n",
    "count_ser.reset_index(level=0, inplace=True)\n",
    "count_ser.columns=[cst.AREA_NUM,'num_establishments']\n",
    "count_ser[cst.AREA_NUM] = count_ser[cst.AREA_NUM].astype(str)\n",
    "count_ser.sort_values('num_establishments');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heatmap_chicago = maps.heat_map(count_ser, \"Number of Establishments\", cst.AREA_NUM, 'num_establishments')\n",
    "heatmap_chicago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of the map**\n",
    "\n",
    "The heatmap above shows the number of establishments on the area we want to analyze. Same as before, we observe that the more we approach the city center, the higher the number of establishments. Indeed, the number of inspections is correlated with the number of establishments; the more establishments we have on a given area, the higher the inspections will be on that area. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average risk per inspection for each area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_to_num(x):\n",
    "    if x == 'Risk 3 (Low)':\n",
    "        return 1\n",
    "    if x == 'Risk 2 (Medium)':\n",
    "        return 2\n",
    "    if x == 'Risk 1 (High)':\n",
    "        return 3\n",
    "    if x == 'All':\n",
    "        return 2\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "risk_DF = food_inspections_DF.copy()\n",
    "risk_DF = risk_DF.dropna(subset=['risk'])\n",
    "risk_DF['risk'] = risk_DF['risk'].apply(lambda x : risk_to_num(x)).astype(int)\n",
    "risk_ser = risk_DF.groupby(cst.AREA_NUM)['risk'].mean().to_frame()\n",
    "risk_ser.reset_index(level=0, inplace=True)\n",
    "risk_ser\n",
    "risk_ser[cst.AREA_NUM] = risk_ser[cst.AREA_NUM].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heatmap_chicago = maps.heat_map(risk_ser, \"Average risk\", cst.AREA_NUM, 'risk')\n",
    "heatmap_chicago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of the map**\n",
    "\n",
    "The heatmap above shows the average risk of the establishments for each community area. The risk of an establishment shows how it could potentially affect the public's health with 1 being the highest and 3 the lowest. Furthermore, **high risk establishments are inspected more frequently that low risk establishments**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of inspections per establishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspections_per_est = inspection_counts.merge(count_ser, left_on=cst.AREA_NUM, right_on=cst.AREA_NUM).drop(['index'], axis = 1)\n",
    "inspections_per_est = inspection_counts.merge(count_ser, left_on=cst.AREA_NUM, right_on=cst.AREA_NUM)\n",
    "inspections_per_est['inspections_per_establishment'] = inspections_per_est.apply(lambda x : x.num_inspections/x.num_establishments, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heatmap_chicago = maps.heat_map(inspections_per_est, \"Number of Inspections per Establishment\", cst.AREA_NUM, 'inspections_per_establishment')\n",
    "heatmap_chicago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of the map**\n",
    "\n",
    "The heatmap above shows the number of inspections per establishment and as highlighted before, we can see that the establishments with a high number of inspections present high average risks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspections by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "inspection_by_year = food_inspections_DF.copy()\n",
    "inspection_by_year['inspection_date'] = pd.DatetimeIndex(inspection_by_year['inspection_date']).year\n",
    "inspection_by_grouped =  inspection_by_year.groupby([cst.AREA_NUM, 'inspection_date']).size().to_frame()\n",
    "inspection_by_grouped = inspection_by_grouped.unstack()\n",
    "inspection_by_grouped.columns = inspection_by_grouped.columns.get_level_values(1)\n",
    "inspection_by_grouped.reset_index(level=0, inplace=True)\n",
    "#inspection_by_grouped = inspection_by_grouped.drop(columns='inspection_date')\n",
    "inspection_by_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# i have no idea why this does not work...i honsetly thinkt that the plugin is simply broken\n",
    "# fixed something by adding 'import folium.plugins' in chicago_map_handler.py but still can't see the heatmap...\n",
    "maps.timed_heatmap(inspection_by_grouped, areas_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each establishment can receive a violation number between 1-44 or 70 and the number is followed by a specific description of the findings that caused the violation to be issued. The higher the number, the better the description. Establishments collecting only high numbers might probably pass whereas the others might probably fail.\n",
    "\n",
    "An inspection of an establishment can pass, pass with conditions or fail depending on these numbers. The 'pass' condition is given when the establishment were found to have no critical or serious violations. Establishments that received a 'pass with conditions' were found to have critical or serious violations but these violations were corrected during the inspection. Finally, the 'fail' condition is issued when the establishment were found to have critical or serious violations that were not correctable during the inspection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Socioeconomic stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['community_area_num', 'community_area_name', 'housing_crowded_perc',\n",
       "       'housholds_below_poverty_perc', 'aged_16_or_more_unemployed_perc',\n",
       "       'aged_25_or_more_without_high_school_diploma_perc',\n",
       "       'aged_under_18_or_over_64_perc', 'per_capita_income', 'hardship_idx',\n",
       "       'life_exp_1990', 'lower_95_perc_CI_1990', 'upper_95_perc_CI_1990',\n",
       "       'life_exp_2000', 'lower_95_perc_CI_2000', 'upper_95_perc_CI_2000',\n",
       "       'life_exp_2010', 'lower_95_perc_CI_2010', 'upper_95_perc_CI_2010'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select one of the following columns for the heatmap\n",
    "socio_life_merged_DF[cst.AREA_NUM] = socio_life_merged_DF[cst.AREA_NUM].astype(str)\n",
    "socio_life_merged_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_chicago = maps.heat_map(socio_life_merged_DF, \"Life expectancy 2010\",cst.AREA_NUM ,'life_exp_2010')\n",
    "heatmap_chicago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'basic-stats'></a>\n",
    "## Basic Statistics\n",
    "\n",
    "We report some statistics on the various dataframes.\n",
    "\n",
    "[Table of Contents](#up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = socio_life_merged_DF[cst.SOCIOECONOMIC_METRICS].corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_metrics = set(['housing_crowded_perc', 'housholds_below_poverty_perc', 'aged_16_or_more_unemployed_perc', \n",
    "               'aged_25_or_more_without_high_school_diploma_perc', 'hardship_idx', 'aged_under_18_or_over_64_perc'])\n",
    "good_metrics = set(['per_capita_income', 'life_exp_2010' ])\n",
    "sign_kept = True\n",
    "\n",
    "for c1 in cst.SOCIOECONOMIC_METRICS:\n",
    "    for c2 in cst.SOCIOECONOMIC_METRICS:\n",
    "        if (c1 in bad_metrics and c2 in bad_metrics) or (c1 in good_metrics and c2 in good_metrics):\n",
    "            if corr[c1][c2] < 0:\n",
    "                sign_kept = False\n",
    "        elif (c1 in bad_metrics and c2 in good_metrics) or (c1 in good_metrics and c2 in bad_metrics):\n",
    "            if corr[c1][c2] > 0:\n",
    "                sign_kept = False\n",
    "print(sign_kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set correlation between each variable and itself to None in order to ignore it later\n",
    "for c in corr.columns:\n",
    "    corr[c][c] = None \n",
    "    \n",
    "corrmax =pd.DataFrame(corr.idxmax()).rename({0: 'Strongest positive correlation'}, axis = 1)\n",
    "corrmax['Correlation value'] = corr.max()\n",
    "corrmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmin =pd.DataFrame(corr.idxmin()).rename({0: 'Strongest negative correlation'}, axis = 1)\n",
    "corrmin['Correlation value'] = corr.min()\n",
    "corrmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the above correlations, we notice certain things: Firstly, we can classify the indicators between good (life expectancy and per capita income) and bad (percentage of crowded houses, percentage of below porverty households, percentage of over 16 unemployed people, percentage fo over 25 people without a high school diploma, the hardship index, and the percentage of people under 18 and over 64), and the correlation between indicators either both good or both bad will always be positive, whereas the correlation between a good and a bad indicator will always be negative. \n",
    "\n",
    "We also notice that the percentage of people under 18 or over 64 is a strong negative indicator: it is more negatively correlated to per capita income than, for example, the percentage of houses living below the poverty line. \n",
    "\n",
    "It is indeed quite surprising that per capita average income is not more correlated to the percentage of houses living below the poverty line (correlation is -0.56). We plot the 2 metrics in order to see this:\n",
    "\n",
    "One reason the linear correlation is so low is that the relationship is exponential. Also, the top 5 highest per capita neighbourhoods are not in the top 15 lowest poor households percentage. TODO: why does this happen??'?!!! where (very downtown). What are some other indicators in this 'mixed' (rich and poor people) neighbourhoods?? This is a cool direction to go in i think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = vincent.Scatter(socio_life_merged_DF[['per_capita_income','housholds_below_poverty_perc']], iter_idx = 'housholds_below_poverty_perc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter.axis_titles(x='Percentage of households below the poverty line', y='per_capita_income')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: You can not click in the above plot. It'd be cool to be able to click and see the name of a neighbourhood. How can we do this with vincent? I kknow how with plotly but they said to use vincent. \n",
    "\n",
    "CONTINUATION: finish socioeconomic things (average, stard deviation, max min. How many people live in poor areas, how many people live in bad areas, etc.). Maybe classify areas in 4 manually. Plot where al this is.\n",
    "\n",
    "Once socioeconomic things are done, let's look at food things (inspections: where are they happening)?\n",
    "\n",
    "Finally, look at correlation between the 2.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
